# config/sentiment_analysis_config.yaml

# Main sentiment analysis configuration
sentiment_analysis:
  # Model selection and parameters
  model:
    # Primary sentiment model to use - Options: "finbert", "vader", "ensemble"
    type: "ensemble"

    # FinBERT configuration
    finbert:
      model_path: "ProsusAI/finbert"  # Use the Hugging Face hosted FinBERT model
      use_pipeline: true             # Load model via transformers pipeline
      pipeline_task: "text-classification"
      max_sequence_length: 512                # Maximum token sequence length
      batch_size: 16                          # Batch size for inference
      quantization: true                      # Use quantized model for faster inference

    # VADER configuration
    vader:
      finance_lexicon_path: "config/finance_lexicon.txt"  # Path to finance-specific lexicon

    # Ensemble configuration
    ensemble:
      weights:
        finbert: 0.7  # Weight for FinBERT in ensemble
        vader: 0.3    # Weight for VADER in ensemble

  # Reddit specific configuration
  reddit:
    # Subreddit calibration values to adjust for subreddit-specific sentiment baselines
    subreddit_calibration:
      wallstreetbets:
        bias: 0.1    # Positive bias - slightly more bullish baseline
        scale: 1.2   # Higher volatility in sentiment
      stocks:
        bias: 0.0    # Neutral baseline
        scale: 1.0   # Standard scale
      investing:
        bias: -0.05  # Slightly more bearish baseline
        scale: 0.9   # Lower volatility in sentiment
      options:
        bias: 0.15   # More bullish baseline
        scale: 1.3   # Higher volatility
      pennystocks:
        bias: 0.2    # More bullish baseline
        scale: 1.5   # Much higher volatility

    # Confidence scoring based on post metadata
    confidence_factors:
      min_score: 5           # Minimum score (upvotes) for full confidence
      min_comments: 3        # Minimum comments for full confidence
      upvote_ratio_threshold: 0.6  # Upvote ratio threshold for good consensus

    # Post quality scoring
    quality_scoring:
      min_text_length: 50    # Minimum text length for quality content
      ideal_text_length: 500 # Ideal text length for maximum quality score
      emoji_penalty: 0.1     # Penalty for excessive emoji use
      url_bonus: 0.05        # Bonus for including URLs (references)

    # Ticker extraction
    ticker_extraction:
      min_length: 1          # Minimum ticker length
      max_length: 5          # Maximum ticker length
      require_dollar_sign: true  # Require $ before ticker symbol
      exclude_common_words: true # Exclude common words that look like tickers

    # Meme stock language
    meme_language:
      dictionary_path: "config/meme_stock_terms.json"  # Path to meme stock dictionary
      impact_weight: 0.3     # Weight of meme language in sentiment

  # Preprocessing configuration
  preprocessing:
    lowercase: true
    remove_urls: true
    remove_html: true
    remove_mentions: true
    extract_hashtags: true
    convert_emojis: true
    normalize_unicode: true
    remove_special_chars: true
    remove_numbers: false
    remove_stopwords: true
    use_finance_stopwords: true
    use_lemmatization: true

  # Performance settings
  performance:
    max_workers: 8                # Number of worker threads for parallel processing
    cache_enabled: true           # Enable results caching
    cache_ttl: 3600               # Cache time-to-live in seconds (1 hour)
    logging_level: "INFO"         # Logging level (DEBUG, INFO, WARNING, ERROR)
    batch_size: 100               # Process posts in batches of this size
    window_duration_ms: 500       # Processing window duration in milliseconds

# Aggregation configuration
aggregation:
  # Source weighting configuration
  source_weights:
    twitter: 0.30       # Weight for Twitter sentiment
    reddit: 0.25        # Weight for Reddit sentiment
    news: 0.40          # Weight for news sentiment
    other: 0.05         # Weight for other sources

  # Time decay parameters
  time_decay_factor: 0.05         # Exponential decay factor for older sentiment
  max_age_hours: 72              # Maximum age to consider for sentiment data

  # Confidence parameters
  confidence_data_points: 20      # Number of data points for full confidence

  # Default parameters
  default_ticker_count: 20        # Number of tickers to include in market sentiment
  max_records: 1000               # Maximum records to retrieve per query

  # Trend detection parameters
  trend:
    min_change_threshold: 0.05    # Minimum change to register as a trend
    shift_detection_threshold: 0.15  # Threshold for significant sentiment shifts

# Kafka configuration for data ingestion
kafka:
  enabled: true
  bootstrap_servers_container: [ "redpanda:29092" ]
  group_id: "sentiment-analysis-group"
  auto_offset_reset: "earliest"
  enable_auto_commit: true
  poll_timeout_ms: 1000
  topics:
    social_media_reddit_validated: "social-media-reddit-posts-validated"
    social_media_reddit_comments_validated: "social-media-reddit-comments-validated"
    twitter: "social-media.twitter"
    news: "finance.news"

# Redis configuration for caching
redis:
  enabled: true
  host: "redis"
  port: 6379
  db: 0
  socket_timeout: 5

# MongoDB configuration for sentiment storage
mongodb:
  connection_host: "mongodb"
  connection_port: 27017
  database: "social_media"
  collection: "reddit_sentiment"
  ticker_collection: "ticker_sentiment"
  auth_username: "mongodb_user"
  auth_password: "mongodb_password"

# Path settings
paths:
  tickers_file: "config/valid_tickers.txt"  # Path to file containing valid ticker symbols
  checkpoint_location: "/tmp/reddit_sentiment_checkpoint"  # Checkpoint location for Spark streaming